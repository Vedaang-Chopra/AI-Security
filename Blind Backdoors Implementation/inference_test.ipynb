{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1ea710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'max_split_size_mb:128,expandable_segments:True'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Path of current working directory (where notebook is running)\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.join(cwd, \"backdoors101\"))\n",
    "\n",
    "import os\n",
    "os.environ.pop(\"PYTORCH_CUDA_ALLOC_CONF\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f90e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'synths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Task   \u001b[38;5;66;03m# repo Task loader (adjust import as needed)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msynths\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_synthesizer  \u001b[38;5;66;03m# or however synths are constructed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_test_dataset  \u001b[38;5;66;03m# pseudo-import - adapt to repo\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'synths'"
     ]
    }
   ],
   "source": [
    "# eval_backdoor.py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tasks.task import Task   # repo Task loader (adjust import as needed)\n",
    "from attack import make_synthesizer  # or however synths are constructed\n",
    "from datasets import make_test_dataset  # pseudo-import - adapt to repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"saved_models/your_run/final_model.pt\"\n",
    "PARAMS_PATH = \"configs/mnist_params.yaml\"  # used to construct same Task/synth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) load model + task (so we know preprocessing)\n",
    "task = Task.from_params(PARAMS_PATH, train=False)  # or use Helper to build Task\n",
    "model = task.model\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 2) build test dataloader (clean)\n",
    "test_ds = task.make_test_dataset()  # repo-specific helper\n",
    "dl = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "# 3) get synthesizer (same one used in train)\n",
    "synth = make_synthesizer(PARAMS_PATH)  # construct with same params\n",
    "\n",
    "total = 0\n",
    "clean_correct = 0\n",
    "asr_count = 0\n",
    "asr_total = 0\n",
    "\n",
    "target_label = task.params.attack.target_label  # the attacker target\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dl:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        # clean\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        clean_correct += (preds == y).sum().item()\n",
    "\n",
    "        # triggered version (synthesizer takes a batch and returns backdoored x)\n",
    "        x_t = synth.apply_trigger_batch(x)   # repo-specific API\n",
    "        logits_t = model(x_t)\n",
    "        preds_t = logits_t.argmax(dim=1)\n",
    "        # count how many triggered examples mapped to target_label\n",
    "        asr_count += (preds_t == target_label).sum().item()\n",
    "        asr_total += x_t.shape[0]\n",
    "\n",
    "        total += x.shape[0]\n",
    "\n",
    "clean_acc = clean_correct / total\n",
    "asr = asr_count / asr_total\n",
    "print(f\"Clean accuracy: {clean_acc:.4f} | ASR (target {target_label}): {asr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params used during training\n",
    "with open(\"./config_loss.yml\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "params['name'] = 'mnist_eval'\n",
    "params['log'] = False\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17276afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate helper (it builds task/model)\n",
    "helper = Helper(params)\n",
    "task = helper.task\n",
    "model = task.model\n",
    "device = helper.params.device\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bc31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved model if training saved one\n",
    "# helper.load_model('paper_presentation/saved_models/model_MNIST_Sep.30_22.41.29_mnist_loss_test_run/model_last.pt.tar')   # adjust path if needed\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean accuracy\n",
    "correct = total = 0\n",
    "test_loader = DataLoader(task.test_dataset, batch_size=helper.params.test_batch_size)\n",
    "for X, y in test_loader:\n",
    "    X = X.to(device); y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        preds = logits.argmax(dim=1)\n",
    "    correct += (preds == y).sum().item()\n",
    "    total += y.size(0)\n",
    "print(\"Clean acc:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17276afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR: poison the same test samples with the same synthesizer used in training\n",
    "backdoor_label = helper.params.backdoor_label\n",
    "correct = total = 0\n",
    "synth = helper.attack.synthesizer  # repo typically exposes attack.synthesizer\n",
    "for X, y in test_loader:\n",
    "    # synth.synthesize_batch is pseudocode â€” replace with actual synthesizer API\n",
    "    Xp = synth.synthesize_batch(X, target_label=backdoor_label)  # adapt to repo method\n",
    "    Xp = Xp.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(Xp)\n",
    "        preds = logits.argmax(dim=1)\n",
    "    correct += (preds == backdoor_label).sum().item()\n",
    "    total += preds.size(0)\n",
    "print(\"ASR:\", correct / total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_presentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
